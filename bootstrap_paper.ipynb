{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pickle\n",
    "from scipy.stats import skew\n",
    "import seaborn as sns\n",
    "import PI_class_EnbPI_journal as EnbPI\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "from pyod.models.pca import PCA\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.knn import KNN  # kNN detector\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import neighbors\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "import utils_EnbPI_journal as util\n",
    "from matplotlib.lines import Line2D  # For legend handles\n",
    "import calendar\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "import itertools\n",
    "import importlib\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "importlib.reload(sys.modules[\"PI_class_EnbPI_journal\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"Sec 5.2 Figure 3 on comparing with time-series methods\n",
    "   Silimar results in the appendix are included as well\"\"\"\n",
    "\n",
    "\n",
    "def big_transform(CA_cities, current_city, one_dim, d, train_size):\n",
    "    # Used for California data\n",
    "    # Next, merge these data (so concatenate X_t and Y_t for one_d or not)\n",
    "    # Return [X_train, X_test, Y_train, Y_test] from data_x and data_y\n",
    "    # Data_x is either multivariate (direct concatenation)\n",
    "    # or univariate (transform each series and THEN concatenate the transformed series)\n",
    "    big_X_train = []\n",
    "    big_X_predict = []\n",
    "    for city in CA_cities:\n",
    "        data = eval(f\"data{city}\")  # Pandas DataFrame\n",
    "        data_x = data.loc[:, data.columns != \"DHI\"]\n",
    "        data_y = data[\"DHI\"]\n",
    "        data_x_numpy = data_x.to_numpy()  # Convert to numpy\n",
    "        data_y_numpy = data_y.to_numpy()  # Convert to numpy\n",
    "        X_train = data_x_numpy[:train_size, :]\n",
    "        X_predict = data_x_numpy[train_size:, :]\n",
    "        Y_train_del = data_y_numpy[:train_size]\n",
    "        Y_predict_del = data_y_numpy[train_size:]\n",
    "        if city == current_city:\n",
    "            Y_train = Y_train_del\n",
    "            Y_predict = Y_predict_del\n",
    "        if one_dim:\n",
    "            X_train, X_predict, Y_train_del, Y_predict_del = util.one_dimen_transform(\n",
    "                Y_train_del, Y_predict_del, d=d\n",
    "            )\n",
    "            big_X_train.append(X_train)\n",
    "            big_X_predict.append(X_predict)\n",
    "            if city == current_city:\n",
    "                Y_train = Y_train_del\n",
    "        else:\n",
    "            big_X_train.append(X_train)\n",
    "            big_X_predict.append(X_predict)\n",
    "    X_train = np.hstack(big_X_train)\n",
    "    X_predict = np.hstack(big_X_predict)\n",
    "    return [X_train, X_predict, Y_train, Y_predict]\n",
    "\n",
    "\n",
    "# Read data and initialize parameters\n",
    "result_type = \"Fig3\"\n",
    "response_ls = {\n",
    "    \"Solar_Atl\": \"DHI\",\n",
    "    \"Palo_Alto\": \"DHI\",\n",
    "    \"Wind_Austin\": \"MWH\",\n",
    "    \"green_house\": 15,\n",
    "    \"appliances\": \"Appliances\",\n",
    "    \"Beijing_air\": \"PM2.5\",\n",
    "}\n",
    "if result_type == \"Fig3\":\n",
    "    # Figure 3\n",
    "    max_data_size = 10000\n",
    "    dataSolar_Atl = util.read_data(3, \"Data/Solar_Atl_data.csv\", max_data_size)\n",
    "    Data_name = [\"Solar_Atl\"]\n",
    "    CA_energy_data = False\n",
    "elif result_type == \"AppendixB3\":\n",
    "    # Results in Appendix B.3\n",
    "    CA_cities = [\n",
    "        \"Fremont\",\n",
    "        \"Milpitas\",\n",
    "        \"Mountain_View\",\n",
    "        \"North_San_Jose\",\n",
    "        \"Palo_Alto\",\n",
    "        \"Redwood_City\",\n",
    "        \"San_Mateo\",\n",
    "        \"Santa_Clara\",\n",
    "        \"Sunnyvale\",\n",
    "    ]\n",
    "    for city in CA_cities:\n",
    "        globals()[\"data%s\" % city] = util.read_CA_data(f\"Data/{city}_data.csv\")\n",
    "    Data_name = [\"Palo_Alto\"]\n",
    "    CA_energy_data = True\n",
    "else:\n",
    "    # Results in Appendix B.4\n",
    "    datagreen_house = util.read_data(0, \"Data/green_house_data.csv\", max_data_size)\n",
    "    dataappliances_data = util.read_data(1, \"Data/appliances_data.csv\", max_data_size)\n",
    "    dataBeijing_air = util.read_data(\n",
    "        2, \"Data/Beijing_air_Tiantan_data.csv\", max_data_size\n",
    "    )\n",
    "    Data_name = [\"green_house\", \"appliances\", \"Beijing_air\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_alpha = 0.0001\n",
    "max_alpha = 10\n",
    "ridge_cv = RidgeCV(alphas=np.linspace(min_alpha, max_alpha, 10))\n",
    "lasso_cv = LassoCV(alphas=np.linspace(min_alpha, max_alpha, 10))\n",
    "random_forest = RandomForestRegressor(\n",
    "    n_estimators=10, criterion=\"squared_error\", bootstrap=False, max_depth=2, n_jobs=-1\n",
    ")\n",
    "extra_trees = ExtraTreesRegressor(n_estimators=10, criterion=\"squared_error\", bootstrap=False, max_depth=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSolar_Atl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(list(dataSolar_Atl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha_ls = [0.01, 0.05, 0.10, 0.15, 0.20, 0.25, 0.50, 0.75, 0.80, 0.85, 0.90, 0.95, 0.99]#np.linspace(0.05, 0.50, 10)\n",
    "alpha_ls = [0.01, 0.10, 0.20, 0.50, 0.80, 0.90, 0.99]#np.linspace(0.05, 0.50, 10)\n",
    "stride = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(sys.modules['PI_class_EnbPI_journal'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# First run time-series methods\n",
    "for data_name in Data_name:\n",
    "    one_dim = True\n",
    "    # `d` is the num_lookbacks for AR-transformer\n",
    "    d = 24\n",
    "    itrial = 0\n",
    "    data = eval(f'data{data_name}')  # Pandas DataFrame\n",
    "    data_x = data.loc[:, data.columns != response_ls[data_name]]\n",
    "    data_y = data[response_ls[data_name]]\n",
    "    total_data_points = data_x.shape[0]\n",
    "    train_size = int(0.2 * total_data_points)\n",
    "    results_ts = pd.DataFrame(columns=['itrial', 'dataname',\n",
    "                                       'method', 'alpha', 'coverage', 'width'])\n",
    "    np.random.seed(98765 + itrial)\n",
    "    for alpha in alpha_ls:\n",
    "        print(f'At trial # {itrial} and alpha={alpha}')\n",
    "        print(f'For {data_name}')\n",
    "        if CA_energy_data:\n",
    "            X_train, X_predict, Y_train, Y_predict = big_transform(\n",
    "                Data_name, data_name, one_dim, d, train_size)\n",
    "        else:\n",
    "            # for one-step ahead forecasting. \n",
    "            #TODO: generalize this for multistep forecasting\n",
    "            data_y = data_y.shift(-1)\n",
    "            data_y.dropna(inplace=True)\n",
    "            data_x.drop(data_x.tail(1).index, inplace=True)\n",
    "\n",
    "            data_x_numpy = data_x.to_numpy()  # Convert to numpy\n",
    "            data_y_numpy = data_y.to_numpy()  # Convert to numpy\n",
    "            X_train = data_x_numpy[:train_size, :]\n",
    "            X_predict = data_x_numpy[train_size:, :]\n",
    "            Y_train = data_y_numpy[:train_size]\n",
    "            Y_predict = data_y_numpy[train_size:]\n",
    "        \n",
    "        ridge_results = EnbPI.prediction_interval(\n",
    "            fit_func=ridge_cv,  X_train=X_train, X_predict=X_predict, Y_train=Y_train, Y_predict=Y_predict, bootstrap_type=\"circular\", block_length=24)\n",
    "        # For ARIMA and other time-series methods, only run once\n",
    "        result_ts = ridge_results.run_experiments(\n",
    "            alpha, stride, data_name, itrial, none_CP=True)\n",
    "        result_ts.rename(columns={'train_size': 'alpha'}, inplace=True)\n",
    "        if CA_energy_data:\n",
    "            result_ts['alpha'].replace(\n",
    "                train_size - d, alpha, inplace=True)\n",
    "        else:\n",
    "            result_ts['alpha'].replace(\n",
    "                train_size, alpha, inplace=True)\n",
    "        results_ts = pd.concat([results_ts, result_ts])\n",
    "        results_ts.to_csv(\n",
    "            f'Results/{data_name}_many_alpha_new_tseries.csv', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then run conformal-related methods\n",
    "stride = 1\n",
    "# `d` is num_lookbacks for AR-transformer\n",
    "d = 24\n",
    "miss_test_idx = []\n",
    "#alpha = 0.1\n",
    "tot_trial = 1  # For CP methods that randomizes (for EnbPI, should be set to 1)\n",
    "B = 25  # number of bootstrap samples\n",
    "#rnn = True\n",
    "methods = ['Ensemble']#, 'ICP', 'Weighted_ICP']\n",
    "bootstrap_types = [\"circular\", \"stationary\", \"moving\", \"random\"]#, \"random\"]\n",
    "block_lengths = [6, 12, 24, 48, 60, 72]\n",
    "# NOTE, if want to run J+aB (Kim et al. 2020), then let\n",
    "# methods = ['Ensemble', 'ICP', 'Weighted_ICP', 'JaB']\n",
    "\n",
    "#the one_dim thing just means whether to use the\n",
    "for one_dim in [True, False]:\n",
    "    for data_name in Data_name:\n",
    "        data = eval(f'data{data_name}')  # Pandas DataFrame\n",
    "        data_x = data.loc[:, data.columns != response_ls[data_name]]\n",
    "        data_y = data[response_ls[data_name]]\n",
    "        total_data_points = data_x.shape[0]\n",
    "        #train_size = int(0.10 * total_data_points)\n",
    "        Train_size = np.linspace(0.1 * total_data_points,\n",
    "                                 0.2 * total_data_points, 2).astype(int)\n",
    "        #Train_size = [Train_size[0], Train_size[4], Train_size[8]]\n",
    "        results = pd.DataFrame(columns=['itrial', 'dataname', 'muh_fun',\n",
    "                                        'method', \"train_size\", 'alpha', 'coverage', 'width', \"bootstrap_type\", \"block_length\"])\n",
    "        for itrial in range(tot_trial):\n",
    "            np.random.seed(98765 + itrial)\n",
    "            for train_size in Train_size:\n",
    "                # Note, this is necessary because a model may \"remember the past\"\n",
    "                #nnet = util.keras_mod()\n",
    "                #rnnet = util.keras_rnn()\n",
    "                print(f'For {data_name}')\n",
    "                print(f'At trial # {itrial} and train_size={train_size}')\n",
    "                if CA_energy_data:\n",
    "                    X_train, X_predict, Y_train, Y_predict = big_transform(\n",
    "                        Data_name, data_name, one_dim, d, train_size)\n",
    "                else:\n",
    "                    # for one-step ahead forecasting. \n",
    "                    #TODO: generalize this for multistep forecasting\n",
    "                    data_y = data_y.shift(-1)\n",
    "                    data_y.dropna(inplace=True)\n",
    "                    data_x.drop(data_x.tail(1).index, inplace=True)\n",
    "\n",
    "                    data_x_numpy = data_x.to_numpy()  # Convert to numpy\n",
    "                    data_y_numpy = data_y.to_numpy()  # Convert to numpy\n",
    "\n",
    "                    X_train = data_x_numpy[:train_size, :]\n",
    "                    X_predict = data_x_numpy[train_size:, :]\n",
    "                    Y_train = data_y_numpy[:train_size]\n",
    "                    Y_predict = data_y_numpy[train_size:]\n",
    "                    if one_dim:\n",
    "                        X_train, X_predict, Y_train, Y_predict = util.one_dimen_transform(\n",
    "                            Y_train, Y_predict, d=d)\n",
    "\n",
    "                for bootstrap_type in tqdm(bootstrap_types):\n",
    "                    for block_length in tqdm(block_lengths):\n",
    "                        lasso_results = EnbPI.prediction_interval(fit_func=lasso_cv,  X_train=X_train, X_predict=X_predict, Y_train=Y_train, Y_predict=Y_predict, bootstrap_type=bootstrap_type, block_length=block_length, random_seed=B+itrial)\n",
    "                        lasso_results.fit_bootstrap_models_online(B, miss_test_idx)\n",
    "                        \n",
    "                        ridge_results = EnbPI.prediction_interval(\n",
    "                            fit_func=ridge_cv,  X_train=X_train, X_predict=X_predict, Y_train=Y_train, Y_predict=Y_predict, bootstrap_type=bootstrap_type, block_length=block_length, random_seed=B+itrial)\n",
    "                        ridge_results.fit_bootstrap_models_online(B, miss_test_idx)\n",
    "                        \n",
    "                        rf_results = EnbPI.prediction_interval(\n",
    "                            fit_func=random_forest,  X_train=X_train, X_predict=X_predict, Y_train=Y_train, Y_predict=Y_predict, bootstrap_type=bootstrap_type, block_length=block_length, random_seed=B+itrial)\n",
    "                        rf_results.fit_bootstrap_models_online(B, miss_test_idx)\n",
    "\n",
    "                        et_results = EnbPI.prediction_interval(\n",
    "                            fit_func=extra_trees,  X_train=X_train, X_predict=X_predict, Y_train=Y_train, Y_predict=Y_predict, bootstrap_type=bootstrap_type, block_length=block_length, random_seed=B+itrial)\n",
    "                        et_results.fit_bootstrap_models_online(B, miss_test_idx)\n",
    "\n",
    "                        for alpha in alpha_ls:\n",
    "                            # Note, this is necessary because a model may \"remember the past\"\n",
    "                            print(f'At trial # {itrial} and alpha={alpha}')\n",
    "                            print(f'For {data_name}')\n",
    "                            \n",
    "                            # CP Methods\n",
    "                            print(f'regressor is {lasso_cv.__class__.__name__}')\n",
    "                            result_lasso = lasso_results.run_experiments(\n",
    "                                alpha, stride, data_name, itrial, methods=methods)\n",
    "                            \n",
    "                            print(f'regressor is {extra_trees.__class__.__name__}')\n",
    "                            result_et = et_results.run_experiments(\n",
    "                                alpha, stride, data_name, itrial, methods=methods)\n",
    "                            \n",
    "                            print(f'regressor is {ridge_cv.__class__.__name__}')\n",
    "                            result_ridge = ridge_results.run_experiments(\n",
    "                                alpha, stride, data_name, itrial, methods=methods)\n",
    "                            \n",
    "                            print(f'regressor is {random_forest.__class__.__name__}')\n",
    "                            result_rf = rf_results.run_experiments(\n",
    "                                alpha, stride, data_name, itrial, methods=methods)\n",
    "                            \n",
    "                            results_now = pd.concat(\n",
    "                                    [result_lasso, result_ridge, result_rf, result_et])\n",
    "                            results_now.rename(\n",
    "                                columns={'train_size': 'alpha'}, inplace=True)\n",
    "                            \n",
    "                            if one_dim:\n",
    "                                results_now['alpha'].replace(\n",
    "                                    train_size - d, alpha, inplace=True)\n",
    "                            else:\n",
    "                                results_now['alpha'].replace(\n",
    "                                    train_size, alpha, inplace=True)\n",
    "                            \n",
    "                            results_now[\"block_length\"] = block_length\n",
    "                            results_now[\"bootstrap_type\"] = bootstrap_type\n",
    "                            results = pd.concat([results, results_now])\n",
    "\n",
    "        if one_dim:\n",
    "            results.to_csv(\n",
    "                f'Results/{data_name}_many_alpha_many_train_new_1d.csv', index=False)\n",
    "        else:\n",
    "            results.to_csv(\n",
    "                f'Results/{data_name}_many_alpha_many_train_new.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results[(results[\"muh_fun\"]==\"RidgeCV\") & (results[\"alpha\"]==0.05)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results[(results[\"muh_fun\"]==\"RidgeCV\") & (results[\"alpha\"]==0.05)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_ar = pd.read_csv(f'Results/{data_name}_many_alpha_new_1d.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_ar[(results_ar[\"muh_fun\"]==\"RandomForestRegressor\") & (results_ar[\"alpha\"]==0.25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boot_samples_idx = util.generate_bootstrap_samples(\n",
    "            len(X_train), len(X_train), B, bootstrap_type=\"nonoverlapping\", block_length=6, random_seed=1)\n",
    "#boot_samples_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_x.drop(data_x.tail(1).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fdfba7de94a4415080a75a4ae256c144e2fa6a9d94f539e4cc23c4bea834d1d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
